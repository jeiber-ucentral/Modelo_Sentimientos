{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ea868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import joblib\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from data_tokenizer import procesar_texto\n",
    "import data_loader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b459981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_datos(tfidf, mensajes=True):\n",
    "    '''\n",
    "    Carga y segmenta los datos en entrenamiento, validacion y prueba dado el tokenizador escogido.\n",
    "    Guarda el scaler para normalizar datos en la prediccion.\n",
    "    Argumentos:\n",
    "        * tfidf: True para usar TF-IDF, False para usar CountVectorizer\n",
    "        * mensajes: Por default es True. Indica si se desea imprimir un diagnostico de cantidad de filas y primeros registros de las bases finales\n",
    "\n",
    "    Retorno:\n",
    "        * x_train, x_test, x_val\n",
    "        * y_train, y_test, y_val\n",
    "    '''\n",
    "    # Cargando datos de acuerdo a tokenizador seleccionado\n",
    "    if tfidf:\n",
    "        vectorizador = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/vectorizador_tfidf.pkl')\n",
    "        x = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/tweets_tfidf.pkl')\n",
    "    else:\n",
    "        vectorizador = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/vectorizador_tf.pkl')\n",
    "        x = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/tweets_tf.pkl')\n",
    "    \n",
    "    y = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/labels.pkl')\n",
    "    \n",
    "    # Division en train, test y validacion\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.10, random_state=42)\n",
    "\n",
    "    if mensajes:\n",
    "        print(\"Dimensiones de X completa:\", x.shape)\n",
    "        print(\"Dimensiones de X train:\", x_train.shape)\n",
    "        print(\"Dimensiones de X test:\", x_test.shape)\n",
    "\n",
    "        print(\"\\nPrimeros registros X test:\")\n",
    "        print(x_test.toarray()[:5])\n",
    "\n",
    "        print(\"\\nPrimeras 5 etiquetas\")\n",
    "        print(y[:5])\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0400ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros del vocabulario y la secuencia\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f7310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo RNN Básico\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=64, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')  # para clasificación binaria\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96757c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilar\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119064c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X completa: (31962, 10000)\n",
      "Dimensiones de X train: (23012, 10000)\n",
      "Dimensiones de X test: (6393, 10000)\n",
      "\n",
      "Primeros registros X test:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Primeras 5 etiquetas\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "Pesos de clase: {np.int64(0): np.float64(0.5374626307922272), np.int64(1): np.float64(7.173316708229426)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Llamada correcta a la función division_datos\n",
    "x_train, x_test, x_val, y_train, y_test, y_val = division_datos(tfidf=True)\n",
    "\n",
    "# Calcular pesos de clase\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Pesos de clase:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar con pesos de clase\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights_dict  # esto balancea las clases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 25s 36ms/step - accuracy: 0.5740 - loss: 0.6887 - val_accuracy: 0.5838 - val_loss: 0.6987\n",
    "Epoch 2/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.5679 - loss: 0.6883 - val_accuracy: 0.5512 - val_loss: 0.6903\n",
    "Epoch 3/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 40s 31ms/step - accuracy: 0.5685 - loss: 0.6832 - val_accuracy: 0.6453 - val_loss: 0.6920\n",
    "Epoch 4/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 42s 34ms/step - accuracy: 0.5556 - loss: 0.7025 - val_accuracy: 0.6551 - val_loss: 0.6896\n",
    "Epoch 5/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 39s 31ms/step - accuracy: 0.5421 - loss: 0.6917 - val_accuracy: 0.6837 - val_loss: 0.6893\n",
    "Epoch 6/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 42s 32ms/step - accuracy: 0.5321 - loss: 0.6974 - val_accuracy: 0.5643 - val_loss: 0.6944\n",
    "Epoch 7/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.5275 - loss: 0.6836 - val_accuracy: 0.3760 - val_loss: 0.7004\n",
    "Epoch 8/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 40s 32ms/step - accuracy: 0.5320 - loss: 0.6802 - val_accuracy: 0.5448 - val_loss: 0.7028\n",
    "Epoch 9/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 40s 31ms/step - accuracy: 0.5288 - loss: 0.6992 - val_accuracy: 0.5977 - val_loss: 0.6917\n",
    "Epoch 10/10\n",
    "699/699 ━━━━━━━━━━━━━━━━━━━━ 42s 32ms/step - accuracy: 0.5492 - loss: 0.6942 - val_accuracy: 0.5360 - val_loss: 0.7064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc699a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en el set de prueba\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Loss en test: {loss:.4f}\")\n",
    "print(f\"Accuracy en test: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "175/175 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.6383 - loss: 0.6963\n",
    "Loss en test: 0.6967\n",
    "Accuracy en test: 0.6397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870eaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver métricas más completas (confusion matrix, precision, recall, F1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Reporte\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5860b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "175/175 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.66      0.77      5203\n",
    "           1       0.09      0.43      0.14       390\n",
    "\n",
    "    accuracy                           0.64      5593\n",
    "   macro avg       0.51      0.54      0.46      5593\n",
    "weighted avg       0.88      0.64      0.73      5593"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f86ed5",
   "metadata": {},
   "source": [
    "Precisión: De los casos que el modelo predijo como clase 1, solo el 9% eran correctos → muchos falsos positivos.\n",
    "\n",
    "Recall: El modelo solo detectó el 43% de los positivos reales → se le están escapando bastantes.\n",
    "\n",
    "F1-score: Balance entre precisión y recall. Muy bajo (0.14) en la clase 1 → modelo no está aprendiendo bien a detectar la clase minoritaria.\n",
    "\n",
    "3410: Verdaderos Negativos (TN) → el modelo acertó con clase 0.\n",
    "\n",
    "1793: Falsos Positivos (FP) → el modelo predijo 1 pero era 0.\n",
    "\n",
    "222: Falsos Negativos (FN) → el modelo predijo 0 pero era 1.\n",
    "\n",
    "168: Verdaderos Positivos (TP) → el modelo acertó con clase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757840c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entrenamiento\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Loss validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece2cb9",
   "metadata": {},
   "source": [
    "Precisión durante el entrenamiento:\n",
    "\n",
    "La precisión de entrenamiento (línea azul) es bastante estable entre 0.53 y 0.55. Eso puede indicar que el modelo está aprendiendo muy lentamente o que está limitado por su capacidad.\n",
    "\n",
    "La precisión de validación (línea naranja) varía mucho y cae bruscamente en algunas épocas (por ejemplo en la época 6).\n",
    "\n",
    "Esta oscilación tan fuerte sugiere alta varianza, lo cual puede ser señal de sobreajuste (overfitting) o que el modelo no está generalizando bien.\n",
    "\n",
    "Pérdida durante el entrenamiento:\n",
    "\n",
    "La pérdida de entrenamiento (línea azul) baja muy lentamente, lo que puede indicar que el modelo no está aprendiendo eficientemente.\n",
    "\n",
    "La pérdida de validación es inestable y en general tiende a subir → lo que también es una señal de overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
