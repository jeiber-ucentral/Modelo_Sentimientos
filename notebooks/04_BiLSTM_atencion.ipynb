{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, Bidirectional, LSTM, Layer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import joblib\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from data_tokenizer import procesar_texto\n",
    "import data_loader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from keras.layers import Dense, LSTM\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def division_datos(tfidf, mensajes=True):\n",
    "    '''\n",
    "    Carga y segmenta los datos en entrenamiento, validacion y prueba dado el tokenizador escogido.\n",
    "    Guarda el scaler para normalizar datos en la prediccion.\n",
    "    Argumentos:\n",
    "        * tfidf: True para usar TF-IDF, False para usar CountVectorizer\n",
    "        * mensajes: Por default es True. Indica si se desea imprimir un diagnostico de cantidad de filas y primeros registros de las bases finales\n",
    "\n",
    "    Retorno:\n",
    "        * x_train, x_test, x_val\n",
    "        * y_train, y_test, y_val\n",
    "    '''\n",
    "    # Cargando datos de acuerdo a tokenizador seleccionado\n",
    "    if tfidf:\n",
    "        vectorizador = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/vectorizador_tfidf.pkl')\n",
    "        x = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/tweets_tfidf.pkl')\n",
    "    else:\n",
    "        vectorizador = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/vectorizador_tf.pkl')\n",
    "        x = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/tweets_tf.pkl')\n",
    "    \n",
    "    y = joblib.load('C:/Users/gerb2/Documents/DEEPLEARNING/taller2_tweets/Modelo_Sentimientos/models/labels.pkl')\n",
    "    \n",
    "    # Division en train, test y validacion\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.10, random_state=42)\n",
    "\n",
    "    if mensajes:\n",
    "        print(\"Dimensiones de X completa:\", x.shape)\n",
    "        print(\"Dimensiones de X train:\", x_train.shape)\n",
    "        print(\"Dimensiones de X test:\", x_test.shape)\n",
    "\n",
    "        print(\"\\nPrimeros registros X test:\")\n",
    "        print(x_test.toarray()[:5])\n",
    "\n",
    "        print(\"\\nPrimeras 5 etiquetas\")\n",
    "        print(y[:5])\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b346499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros del vocabulario y la secuencia\n",
    "max_features = 10000\n",
    "maxlen  = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamada correcta a la función division_datos\n",
    "x_train, x_test, x_val, y_train, y_test, y_val = division_datos(tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo BiLSTM NO EJECUTAR\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=maxlen),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features): # Only one input is needed for the Attention layer\n",
    "        # hidden_with_time_axis = tf.expand_dims(hidden, 1) # Removed, as hidden state is not needed\n",
    "\n",
    "        # Calculate attention weights based only on features\n",
    "        score = tf.nn.tanh(self.W1(features))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector # Return only the context vector\n",
    "\n",
    "\n",
    "\n",
    "# Modelo BiLSTM con atención\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # return_sequences=True para la atención\n",
    "model.add(Attention(64))  # Capa de atención\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a38990",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/3\n",
    "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
    "  warnings.warn(\n",
    "280/280 ━━━━━━━━━━━━━━━━━━━━ 109s 372ms/step - accuracy: 0.9234 - loss: 0.2486 - val_accuracy: 0.9519 - val_loss: 0.1402\n",
    "Epoch 2/3\n",
    "280/280 ━━━━━━━━━━━━━━━━━━━━ 138s 359ms/step - accuracy: 0.9638 - loss: 0.1017 - val_accuracy: 0.9564 - val_loss: 0.1245\n",
    "Epoch 3/3\n",
    "280/280 ━━━━━━━━━━━━━━━━━━━━ 142s 358ms/step - accuracy: 0.9841 - loss: 0.0546 - val_accuracy: 0.9580 - val_loss: 0.1376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {acc}, test Loss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d111621",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy: 0.9570892453193665, test Loss:0.1369503140449524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver métricas más completas (confusion matrix, precision, recall, F1)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Reporte\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce144b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.99      0.98      5203\n",
    "           1       0.77      0.55      0.64       390\n",
    "\n",
    "    accuracy                           0.96      5593\n",
    "   macro avg       0.87      0.77      0.81      5593\n",
    "weighted avg       0.95      0.96      0.95      5593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entrenamiento\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Loss entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Loss validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
